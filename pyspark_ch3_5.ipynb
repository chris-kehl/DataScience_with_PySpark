{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1yWUXgQJwjMgpCl2k7Xuvot4PlhjuiIS_",
      "authorship_tag": "ABX9TyOo7yx6daPBJeqWBGRfIcF1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chris-kehl/DataScience_with_PySpark/blob/main/pyspark_ch3_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Executing pySpark on colab\n",
        "!apt-get update # Update apt-get repository\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # Install Java\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz # Unzip the tgz file.\n",
        "!pip install -q findspark # install findspark. Adds PySpark to the system path during runtime.\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "!ls\n",
        "# Initialize findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Create pySpark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = (SparkSession\n",
        "         .builder\n",
        "         .appName(\"Analyzing the vocabulary of Pride and Justice.\")\n",
        "         .getOrCreate())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUZT82rHes9i",
        "outputId": "3db55e1e-ce2c-48a4-a79f-e3c06209c984"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "\r0% [Waiting for headers] [1 InRelease 0 B/110 kB 0%] [Connected to cloud.r-project.org (18.160.213.1\r                                                                                                    \rHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [1 InRelease 43.1 kB/110 kB 39%] [Waiting for headers] [Connecting to ppa.launchpadcontent.net (1\r                                                                                                    \rGet:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Waiting for headers] [1 InRelease 43.1 kB/110 kB 39%] [Connecting to ppa.launchpadcontent.net (1\r                                                                                                    \rGet:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [673 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,440 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,065 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,751 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,728 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,335 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,810 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [28.1 kB]\n",
            "Fetched 10.2 MB in 2s (4,236 kB/s)\n",
            "Reading package lists... Done\n",
            "drive  sample_data  spark-3.1.1-bin-hadoop3.2  spark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IAfXMmvdcb_",
        "outputId": "3af2c1c7-628d-4a42-ca26-af08e1701c9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "|word|count|\n",
            "+----+-----+\n",
            "| the| 4480|\n",
            "|  to| 4218|\n",
            "|  of| 3711|\n",
            "| and| 3504|\n",
            "| her| 2199|\n",
            "|   a| 1982|\n",
            "|  in| 1909|\n",
            "| was| 1838|\n",
            "|   i| 1749|\n",
            "| she| 1668|\n",
            "+----+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "+-------------+-----+\n",
            "|         word|count|\n",
            "+-------------+-----+\n",
            "|       online|    4|\n",
            "|         some|  203|\n",
            "|        still|   72|\n",
            "|          few|   72|\n",
            "|         hope|  122|\n",
            "|        those|   60|\n",
            "|     cautious|    4|\n",
            "|       lady's|    8|\n",
            "|    imitation|    1|\n",
            "|          art|    3|\n",
            "|      solaced|    1|\n",
            "|       poetry|    2|\n",
            "|    arguments|    5|\n",
            "| premeditated|    1|\n",
            "|      elevate|    1|\n",
            "|       doubts|    2|\n",
            "|    destitute|    1|\n",
            "|    solemnity|    5|\n",
            "|gratification|    1|\n",
            "|    connected|   14|\n",
            "+-------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (\n",
        "    col,\n",
        "    explode,\n",
        "    lower,\n",
        "    regexp_extract,\n",
        "    split,\n",
        ")\n",
        "\n",
        "spark = SparkSession.builder.appName(\n",
        "    \"Analyzing the vocabulary of Pride and Prejudice.\"\n",
        ").getOrCreate()\n",
        "\n",
        "book = spark.read.text(\"/content/drive/MyDrive/DataAnalysisWithPythonAndPySpark-Data-trunk/gutenberg_books/1342-0.txt\")\n",
        "\n",
        "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
        "\n",
        "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
        "\n",
        "words_lower = words.select(lower(col(\"word\")).alias(\"word\"))\n",
        "\n",
        "words_clean = words_lower.select(\n",
        "    regexp_extract(col(\"word\"), \"[a-z']*\", 0).alias(\"word\")\n",
        ")\n",
        "\n",
        "words_nonull = words_clean.where(col(\"word\") != \"\")\n",
        "\n",
        "results = words_nonull.groupby(col(\"word\")).count()\n",
        "\n",
        "results.orderBy(\"count\", ascending=False).show(10)\n",
        "\n",
        "results.show()\n",
        "\n",
        "results.coalesce(1).write.csv(\"./simple_count_single_partition.csv\")"
      ]
    }
  ]
}